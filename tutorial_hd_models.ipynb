{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import grain.python as grain\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import multivariate_t\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from tqdm import trange\n",
    "from onlineEM.core import em_config\n",
    "from onlineEM.models import HDgmm, HDstm, HDlm\n",
    "\n",
    "# Set plot style for professional appearance\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Dimensional Mixture Models Tutorial üöÄ\n",
    "\n",
    "## 1. Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== UTILITY FUNCTIONS ====================\n",
    "\n",
    "\n",
    "def generate_mixture_of_gaussians(num_samples: int, means: list, covariances: list, weights: list) -> np.ndarray:\n",
    "    \"\"\"Generate synthetic data from a mixture of Gaussian distributions.\"\"\"\n",
    "    num_components = len(means)\n",
    "    component_indices = np.random.choice(num_components, size=num_samples, p=weights)\n",
    "    samples = np.zeros((num_samples, len(means[0])))\n",
    "\n",
    "    for i in range(num_components):\n",
    "        indices = component_indices == i\n",
    "        num_samples_component = np.sum(indices)\n",
    "        if num_samples_component > 0:\n",
    "            component_samples = np.random.multivariate_normal(means[i], covariances[i], num_samples_component)\n",
    "            samples[indices] = component_samples\n",
    "    return samples\n",
    "\n",
    "\n",
    "def create_dataloader(X_array: np.ndarray, config) -> grain.DataLoader:\n",
    "    \"\"\"Create a standardized dataloader for training.\"\"\"\n",
    "\n",
    "    class SimpleDataSource(grain.RandomAccessDataSource):\n",
    "        def __init__(self, data: np.ndarray):\n",
    "            self.data = data\n",
    "\n",
    "        def __len__(self) -> int:\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, index: int) -> np.ndarray:\n",
    "            return self.data[index]\n",
    "\n",
    "    data_source = SimpleDataSource(X_array)\n",
    "    sampler = grain.IndexSampler(num_records=len(data_source), num_epochs=config.num_epochs, shuffle=True, seed=42)\n",
    "    transforms = [grain.Batch(batch_size=config.batch_size, drop_remainder=True)]\n",
    "    return grain.DataLoader(data_source=data_source, sampler=sampler, operations=transforms, worker_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and synthetic data setup\n",
    "config = em_config(\n",
    "    n_components=3,\n",
    "    num_features=5,\n",
    "    num_epochs=30,\n",
    "    batch_size=256,\n",
    "    n_first=20000,\n",
    "    # Using automatic reduction detection (empty array) to avoid model issues\n",
    ")\n",
    "\n",
    "# Component parameters: structured variance pattern for clear eigenvalue structure\n",
    "mean1 = np.array([2.0, 4.0, 1.0, 0.5, 0.2])  # Scaled up for better separation\n",
    "covariance1 = np.diag([10.0, 8.0, 0.1, 0.1, 0.1])  # Clear eigenvalue hierarchy\n",
    "weight1 = 0.4\n",
    "\n",
    "mean2 = np.array([6.0, 2.0, 5.0, 1.0, 0.8])\n",
    "covariance2 = np.diag([8.0, 6.0, 4.0, 0.25, 0.25])  # Clear eigenvalue hierarchy\n",
    "weight2 = 0.3\n",
    "\n",
    "mean3 = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "covariance3 = np.diag([3.0, 2.0, 0.08, 0.08, 0.08])  # Clear eigenvalue hierarchy\n",
    "weight3 = 0.3\n",
    "\n",
    "means = [mean1, mean2, mean3]\n",
    "covariances = [covariance1, covariance2, covariance3]\n",
    "weights = [weight1, weight2, weight3]\n",
    "\n",
    "print(f\"üìä Mixture components configured with total weight: {sum(weights):.1f}\")\n",
    "print(\"üîß Using automatic reduction detection with clear eigenvalue structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "num_samples = 100000\n",
    "X_array = generate_mixture_of_gaussians(num_samples, means, covariances, weights)\n",
    "dataloader = create_dataloader(X_array, config)\n",
    "\n",
    "print(f\"üé≤ Generated {num_samples:,} samples from Gaussian mixture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== VISUALIZATION FUNCTIONS ====================\n",
    "\n",
    "\n",
    "def plot_data_overview(X_array: np.ndarray, title: str = \"Dataset Visualization\"):\n",
    "    \"\"\"Create standardized data visualization with scatter plots and distributions.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle(title, fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    # Pairwise scatter plots\n",
    "    combinations = [(0, 1), (0, 2), (1, 2)]\n",
    "    for i, (dim1, dim2) in enumerate(combinations):\n",
    "        ax = axes[0, i]\n",
    "        ax.scatter(X_array[:5000, dim1], X_array[:5000, dim2], alpha=0.6, s=1, c=np.arange(5000), cmap=\"viridis\")\n",
    "        ax.set_xlabel(f\"Dimension {dim1}\")\n",
    "        ax.set_ylabel(f\"Dimension {dim2}\")\n",
    "        ax.set_title(f\"Dims {dim1} vs {dim2}\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Distribution plots\n",
    "    for i in range(3):\n",
    "        ax = axes[1, i]\n",
    "        ax.hist(X_array[:, i], bins=50, alpha=0.7, density=True, color=sns.color_palette()[i])\n",
    "        ax.set_xlabel(f\"Dimension {i}\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.set_title(f\"Distribution of Dim {i}\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"üìä Dataset Summary:\")\n",
    "    print(f\"Mean: {np.mean(X_array, axis=0)}\")\n",
    "    print(f\"Variance: {np.var(X_array, axis=0)}\")\n",
    "\n",
    "\n",
    "# Visualize Gaussian mixture data\n",
    "plot_data_overview(X_array, \"Gaussian Mixture Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== TRAINING FUNCTIONS ====================\n",
    "\n",
    "\n",
    "def train_hd_model(model, dataloader, config):\n",
    "    \"\"\"Unified training function for HD models.\"\"\"\n",
    "\n",
    "    def schedule(k):\n",
    "        return (1 - 10e-10) * (k + 1) ** (-6 / 10)\n",
    "\n",
    "    # Initialize model first to get updated config\n",
    "    tmp_it = iter(dataloader)\n",
    "    X = np.concatenate([next(tmp_it) for _ in range(config.n_first // config.batch_size)])\n",
    "    updated_config, em_params, em_stats = model.init(X, config)\n",
    "\n",
    "    # Create compiled training steps with UPDATED config\n",
    "    @jax.jit\n",
    "    def burnin_step(batch, step, params, stats):\n",
    "        return model.burnin(batch, step, params, stats, updated_config, schedule)\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(batch, step, params, stats):\n",
    "        return model.update(batch, step, params, stats, updated_config, schedule)\n",
    "\n",
    "    # Burn-in phase\n",
    "    burning_iter = iter(dataloader)\n",
    "    for step in trange(2 * config.num_features, desc=\"Burn-in\", colour=\"green\"):\n",
    "        batch = next(burning_iter)\n",
    "        em_stats = burnin_step(batch, step, em_params, em_stats)\n",
    "\n",
    "    # Training phase\n",
    "    train_iter = iter(dataloader)\n",
    "    for step in trange(config.num_epochs, desc=\"Training\", colour=\"blue\"):\n",
    "        for _ in range(config.num_features):\n",
    "            batch = next(train_iter)\n",
    "            em_params, em_stats = train_step(batch, step, em_params, em_stats)\n",
    "\n",
    "    return em_params, em_stats, updated_config\n",
    "\n",
    "\n",
    "# Train HD-GMM model\n",
    "print(\"üèóÔ∏è Training HD-GMM model...\")\n",
    "model_hdgmm = HDgmm()\n",
    "params_hdgmm, stats_hdgmm, config_hdgmm = train_hd_model(model_hdgmm, dataloader, config)\n",
    "print(f\"‚úÖ HD-GMM training completed with reductions: {config_hdgmm.reduction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== EVALUATION FUNCTIONS ====================\n",
    "\n",
    "\n",
    "def find_best_permutation(true_means, pred_means):\n",
    "    \"\"\"Find optimal component permutation using Hungarian algorithm.\"\"\"\n",
    "    n_components = len(true_means)\n",
    "    cost_matrix = np.zeros((n_components, n_components))\n",
    "\n",
    "    for i in range(n_components):\n",
    "        for j in range(n_components):\n",
    "            cost_matrix[i, j] = np.linalg.norm(true_means[i] - pred_means[j])\n",
    "\n",
    "    _, col_indices = linear_sum_assignment(cost_matrix)\n",
    "    return col_indices\n",
    "\n",
    "\n",
    "def permute_parameters(params, permutation):\n",
    "    \"\"\"Apply permutation to HD model parameters.\"\"\"\n",
    "    return type(params)(\n",
    "        pi=params.pi[permutation],\n",
    "        mu=params.mu[permutation],\n",
    "        A=[params.A[i] for i in permutation],\n",
    "        b=params.b[permutation],\n",
    "        D_tilde=[params.D_tilde[i] for i in permutation],\n",
    "        **({\"nu\": params.nu[permutation]} if hasattr(params, \"nu\") else {}),\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_model_performance(true_means, true_weights, true_covariances, pred_params, model_name=\"HD Model\"):\n",
    "    \"\"\"Comprehensive model evaluation with optimal alignment.\"\"\"\n",
    "    # Find optimal permutation and align parameters\n",
    "    perm = find_best_permutation(true_means, pred_params.mu)\n",
    "    aligned_params = permute_parameters(pred_params, perm)\n",
    "\n",
    "    # Calculate errors\n",
    "    mean_errors = [np.linalg.norm(aligned_params.mu[i] - true_means[i]) for i in range(len(true_means))]\n",
    "    weight_error = np.linalg.norm(np.array(true_weights) - aligned_params.pi)\n",
    "\n",
    "    print(f\"\\nüìä {model_name} Performance:\")\n",
    "    print(f\"Mean reconstruction error: {np.mean(mean_errors):.4f} ¬± {np.std(mean_errors):.4f}\")\n",
    "    print(f\"Weight reconstruction error: {weight_error:.4f}\")\n",
    "    print(f\"Optimal permutation: {perm}\")\n",
    "\n",
    "    return aligned_params, mean_errors\n",
    "\n",
    "\n",
    "# Evaluate HD-GMM\n",
    "aligned_params_hdgmm, errors_hdgmm = evaluate_model_performance(means, weights, covariances, params_hdgmm, \"HD-GMM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(\n",
    "    true_means, true_weights, true_covariances, aligned_params, mean_errors, model_name, model_color\n",
    "):\n",
    "    \"\"\"Create comprehensive model comparison visualization.\"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "    fig.suptitle(f\"{model_name}: Ground Truth vs Predictions\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    # Row 1: Mixing weights comparison\n",
    "    ax = axes[0, 0]\n",
    "    x_pos = np.arange(3)\n",
    "    width = 0.35\n",
    "    ax.bar(x_pos - width / 2, true_weights, width, label=\"True\", color=\"gold\", alpha=0.8)\n",
    "    ax.bar(x_pos + width / 2, aligned_params.pi, width, label=model_name, color=model_color, alpha=0.8)\n",
    "    ax.set_xlabel(\"Components\")\n",
    "    ax.set_ylabel(\"Mixing Weights\")\n",
    "    ax.set_title(\"Mixing Weights Comparison\")\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels([\"Comp 1\", \"Comp 2\", \"Comp 3\"])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Row 1: Mean reconstruction errors\n",
    "    ax = axes[0, 1]\n",
    "    bars = ax.bar(range(3), mean_errors, color=model_color, alpha=0.8)\n",
    "    ax.set_xlabel(\"Components\")\n",
    "    ax.set_ylabel(\"L2 Error\")\n",
    "    ax.set_title(\"Mean Vector Reconstruction Error\")\n",
    "    ax.set_xticks(range(3))\n",
    "    ax.set_xticklabels([\"Comp 1\", \"Comp 2\", \"Comp 3\"])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + max(mean_errors) * 0.01,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    # Row 1: Overall model summary metrics\n",
    "    ax = axes[0, 2]\n",
    "    metrics = ['Mean Error', 'Weight Error', 'Avg Reduction']\n",
    "    # Calculate average reduction from config if available\n",
    "    avg_reduction = np.mean([len(aligned_params.A[i]) for i in range(3)])\n",
    "    values = [np.mean(mean_errors),\n",
    "              np.linalg.norm(np.array(true_weights) - aligned_params.pi),\n",
    "              avg_reduction]\n",
    "    bars = ax.bar(metrics, values, color=[model_color, model_color, 'lightblue'], alpha=0.8)\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.set_title(\"Overall Model Performance\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + max(values) * 0.01,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    # Row 2: Mean vectors comparison for each component\n",
    "    component_names = [\"Component 1\", \"Component 2\", \"Component 3\"]\n",
    "    for comp in range(3):\n",
    "        ax = axes[1, comp]\n",
    "        dims = np.arange(len(true_means[0]))\n",
    "        ax.plot(dims, true_means[comp], \"o-\", color=\"gold\", linewidth=3, markersize=8, label=\"True\")\n",
    "        ax.plot(dims, aligned_params.mu[comp], \"s-\", color=model_color, linewidth=2, markersize=6, label=model_name)\n",
    "        ax.set_xlabel(\"Dimension\")\n",
    "        ax.set_ylabel(\"Mean Value\")\n",
    "        ax.set_title(f\"{component_names[comp]} Mean Vector\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xticks(dims)\n",
    "\n",
    "    # Row 3: Eigenvalue spectra for all components\n",
    "    for comp in range(3):\n",
    "        ax = axes[2, comp]\n",
    "        true_eigenvals = np.sort(np.linalg.eigvals(true_covariances[comp]))[::-1]\n",
    "        # Reconstruct eigenvalues from HD parameters\n",
    "        hd_A = np.array(aligned_params.A[comp])\n",
    "        hd_b = aligned_params.b[comp]\n",
    "        n_leading = len(hd_A)\n",
    "        n_tail = len(true_eigenvals) - n_leading\n",
    "        hd_eigenvals = np.concatenate([hd_A, np.full(n_tail, hd_b)])\n",
    "        hd_eigenvals = np.sort(hd_eigenvals)[::-1]\n",
    "\n",
    "        x_pos = np.arange(len(true_eigenvals))\n",
    "        ax.semilogy(x_pos, true_eigenvals, \"o-\", color=\"gold\", linewidth=3, markersize=8, label=\"True\")\n",
    "        ax.semilogy(x_pos, hd_eigenvals, \"s-\", color=model_color, linewidth=2, markersize=6, label=model_name)\n",
    "        ax.set_xlabel(\"Eigenvalue Index\")\n",
    "        ax.set_ylabel(\"Eigenvalue (log scale)\")\n",
    "        ax.set_title(f\"{component_names[comp]} Eigenvalue Spectrum\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xticks(x_pos)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize HD-GMM results\n",
    "plot_model_comparison(means, weights, covariances, aligned_params_hdgmm, errors_hdgmm, \"HD-GMM\", \"lightcoral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Student-t Mixture Models üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mixture_of_student(\n",
    "    num_samples: int, means: list, covariances: list, nus: list, weights: list\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate synthetic data from multivariate Student-t mixture.\"\"\"\n",
    "    num_components = len(means)\n",
    "    component_indices = np.random.choice(num_components, size=num_samples, p=weights)\n",
    "    samples = np.zeros((num_samples, len(means[0])))\n",
    "\n",
    "    for i in range(num_components):\n",
    "        indices = component_indices == i\n",
    "        num_samples_component = np.sum(indices)\n",
    "        if num_samples_component > 0:\n",
    "            component_samples = multivariate_t.rvs(\n",
    "                loc=means[i], shape=covariances[i], df=nus[i], size=num_samples_component\n",
    "            )\n",
    "            samples[indices] = component_samples\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Student-t mixture parameters (same spatial structure, added degrees of freedom)\n",
    "nus = [5.0, 5.0, 5.0]  # Moderate heavy tails\n",
    "X_array_student = generate_mixture_of_student(num_samples, means, covariances, nus, weights)\n",
    "dataloader_student = create_dataloader(X_array_student, config)\n",
    "\n",
    "print(f\"üé≤ Generated {num_samples:,} samples from Student-t mixture with ŒΩ={nus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_overview(X_array_student, \"Student-t Mixture Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HD-STM model\n",
    "print(\"üèóÔ∏è Training HD-STM model...\")\n",
    "model_hdstm = HDstm()\n",
    "params_hdstm, stats_hdstm, config_hdstm = train_hd_model(model_hdstm, dataloader_student, config)\n",
    "print(f\"‚úÖ HD-STM training completed with reductions: {config_hdstm.reduction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate HD-STM with degrees of freedom analysis\n",
    "aligned_params_hdstm, errors_hdstm = evaluate_model_performance(means, weights, covariances, params_hdstm, \"HD-STM\")\n",
    "\n",
    "# Additional analysis for degrees of freedom\n",
    "print(\"\\nüéì Degrees of Freedom Analysis:\")\n",
    "nu_errors = [abs(aligned_params_hdstm.nu[i] - nus[i]) for i in range(3)]\n",
    "print(f\"True ŒΩ: {nus}\")\n",
    "print(f\"Predicted ŒΩ: {aligned_params_hdstm.nu}\")\n",
    "print(f\"ŒΩ reconstruction error: {np.mean(nu_errors):.4f} ¬± {np.std(nu_errors):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stm_comparison(\n",
    "    true_means, true_weights, true_covariances, true_nus, aligned_params, mean_errors, model_name, model_color\n",
    "):\n",
    "    \"\"\"Create comprehensive HD-STM comparison visualization with degrees of freedom.\"\"\"\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 20))\n",
    "    fig.suptitle(f\"{model_name}: Ground Truth vs Predictions\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    # Row 1: Mixing weights, mean errors, and degrees of freedom\n",
    "    ax = axes[0, 0]\n",
    "    x_pos = np.arange(3)\n",
    "    width = 0.35\n",
    "    ax.bar(x_pos - width / 2, true_weights, width, label=\"True\", color=\"gold\", alpha=0.8)\n",
    "    ax.bar(x_pos + width / 2, aligned_params.pi, width, label=model_name, color=model_color, alpha=0.8)\n",
    "    ax.set_xlabel(\"Components\")\n",
    "    ax.set_ylabel(\"Mixing Weights\")\n",
    "    ax.set_title(\"Mixing Weights Comparison\")\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels([\"Comp 1\", \"Comp 2\", \"Comp 3\"])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    ax = axes[0, 1]\n",
    "    bars = ax.bar(range(3), mean_errors, color=model_color, alpha=0.8)\n",
    "    ax.set_xlabel(\"Components\")\n",
    "    ax.set_ylabel(\"L2 Error\")\n",
    "    ax.set_title(\"Mean Vector Reconstruction Error\")\n",
    "    ax.set_xticks(range(3))\n",
    "    ax.set_xticklabels([\"Comp 1\", \"Comp 2\", \"Comp 3\"])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + max(mean_errors) * 0.01,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    # Degrees of freedom comparison\n",
    "    ax = axes[0, 2]\n",
    "    ax.bar(x_pos - width / 2, true_nus, width, label=\"True ŒΩ\", color=\"gold\", alpha=0.8)\n",
    "    ax.bar(x_pos + width / 2, aligned_params.nu, width, label=\"Predicted ŒΩ\", color=model_color, alpha=0.8)\n",
    "    ax.set_xlabel(\"Components\")\n",
    "    ax.set_ylabel(\"Degrees of Freedom (ŒΩ)\")\n",
    "    ax.set_title(\"Degrees of Freedom Comparison\")\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels([\"Comp 1\", \"Comp 2\", \"Comp 3\"])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Row 2: ŒΩ analysis plots\n",
    "    ax = axes[1, 0]\n",
    "    # Scatter plot: True vs Predicted ŒΩ\n",
    "    ax.scatter(true_nus, aligned_params.nu, s=100, alpha=0.7, color=model_color, edgecolors=\"black\")\n",
    "    nu_min = min(min(true_nus), min(aligned_params.nu)) - 0.5\n",
    "    nu_max = max(max(true_nus), max(aligned_params.nu)) + 0.5\n",
    "    ax.plot([nu_min, nu_max], [nu_min, nu_max], \"r--\", alpha=0.7, label=\"Perfect Prediction\")\n",
    "    ax.set_xlabel(\"True ŒΩ\")\n",
    "    ax.set_ylabel(\"Predicted ŒΩ\")\n",
    "    ax.set_title(\"True vs Predicted Degrees of Freedom\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    for i, (true_nu, pred_nu) in enumerate(zip(true_nus, aligned_params.nu)):\n",
    "        ax.annotate(f\"C{i + 1}\", (true_nu, pred_nu), xytext=(5, 5), textcoords=\"offset points\")\n",
    "\n",
    "    ax = axes[1, 1]\n",
    "    # ŒΩ error analysis\n",
    "    nu_errors = [abs(aligned_params.nu[i] - true_nus[i]) for i in range(3)]\n",
    "    bars = ax.bar(range(3), nu_errors, color=model_color, alpha=0.8)\n",
    "    ax.set_xlabel(\"Components\")\n",
    "    ax.set_ylabel(\"Absolute Error in ŒΩ\")\n",
    "    ax.set_title(\"Degrees of Freedom Reconstruction Error\")\n",
    "    ax.set_xticks(range(3))\n",
    "    ax.set_xticklabels([\"Comp 1\", \"Comp 2\", \"Comp 3\"])\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + max(nu_errors) * 0.01,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    # Overall STM performance summary\n",
    "    ax = axes[1, 2]\n",
    "    metrics = ['Mean Error', 'Weight Error', 'ŒΩ Error']\n",
    "    values = [np.mean(mean_errors),\n",
    "              np.linalg.norm(np.array(true_weights) - aligned_params.pi),\n",
    "              np.mean(nu_errors)]\n",
    "    bars = ax.bar(metrics, values, color=[model_color, model_color, 'lightgreen'], alpha=0.8)\n",
    "    ax.set_ylabel(\"Error Value\")\n",
    "    ax.set_title(\"STM Performance Summary\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + max(values) * 0.01,\n",
    "            f\"{height:.3f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=9,\n",
    "        )\n",
    "\n",
    "    # Row 3: Mean vectors comparison\n",
    "    component_names = [\"Component 1\", \"Component 2\", \"Component 3\"]\n",
    "    for comp in range(3):\n",
    "        ax = axes[2, comp]\n",
    "        dims = np.arange(len(true_means[0]))\n",
    "        ax.plot(dims, true_means[comp], \"o-\", color=\"gold\", linewidth=3, markersize=8, label=\"True\")\n",
    "        ax.plot(dims, aligned_params.mu[comp], \"^-\", color=model_color, linewidth=2, markersize=6, label=model_name)\n",
    "        ax.set_xlabel(\"Dimension\")\n",
    "        ax.set_ylabel(\"Mean Value\")\n",
    "        ax.set_title(f\"{component_names[comp]} Mean Vector\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xticks(dims)\n",
    "\n",
    "    # Row 4: Eigenvalue spectra for all components\n",
    "    for comp in range(3):\n",
    "        ax = axes[3, comp]\n",
    "        true_eigenvals = np.sort(np.linalg.eigvals(true_covariances[comp]))[::-1]\n",
    "        hd_A = np.array(aligned_params.A[comp])\n",
    "        hd_b = aligned_params.b[comp]\n",
    "        n_leading = len(hd_A)\n",
    "        n_tail = len(true_eigenvals) - n_leading\n",
    "        hd_eigenvals = np.concatenate([hd_A, np.full(n_tail, hd_b)])\n",
    "        hd_eigenvals = np.sort(hd_eigenvals)[::-1]\n",
    "\n",
    "        x_pos = np.arange(len(true_eigenvals))\n",
    "        ax.semilogy(x_pos, true_eigenvals, \"o-\", color=\"gold\", linewidth=3, markersize=8, label=\"True\")\n",
    "        ax.semilogy(x_pos, hd_eigenvals, \"^-\", color=model_color, linewidth=2, markersize=6, label=model_name)\n",
    "        ax.set_xlabel(\"Eigenvalue Index\")\n",
    "        ax.set_ylabel(\"Eigenvalue (log scale)\")\n",
    "        ax.set_title(f\"{component_names[comp]} Eigenvalue Spectrum\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xticks(x_pos)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize HD-STM results with degrees of freedom analysis\n",
    "plot_stm_comparison(means, weights, covariances, nus, aligned_params_hdstm, errors_hdstm, \"HD-STM\", \"darkorange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Laplace Mixture Models üî∫\n",
    "\n",
    "Multivariate Laplace distributions have sharp peaks and exponential tails, useful for sparse data modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mixture_of_multivariate_laplace(\n",
    "    num_samples: int, means: list, covariances: list, weights: list\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate synthetic data from multivariate Laplace mixture using hierarchical representation.\"\"\"\n",
    "    num_components = len(means)\n",
    "    d = len(means[0])\n",
    "    component_indices = np.random.choice(num_components, size=num_samples, p=weights)\n",
    "    samples = np.zeros((num_samples, d))\n",
    "\n",
    "    for i in range(num_components):\n",
    "        indices = component_indices == i\n",
    "        num_samples_component = np.sum(indices)\n",
    "        if num_samples_component > 0:\n",
    "            # Hierarchical representation: X|W ~ N(Œº, W*Œ£), W ~ Exp(1)\n",
    "            W = np.random.exponential(1.0, size=num_samples_component)\n",
    "            component_samples = np.zeros((num_samples_component, d))\n",
    "            for j in range(num_samples_component):\n",
    "                scaled_cov = W[j] * covariances[i]\n",
    "                component_samples[j] = np.random.multivariate_normal(means[i], scaled_cov)\n",
    "            samples[indices] = component_samples\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Laplace mixture parameters (amplified means for visibility)\n",
    "means_laplace = [mean * 5 for mean in means]  # Scale up for better separation\n",
    "X_array_laplace = generate_mixture_of_multivariate_laplace(num_samples, means_laplace, covariances, weights)\n",
    "dataloader_laplace = create_dataloader(X_array_laplace, config)\n",
    "\n",
    "print(f\"üé≤ Generated {num_samples:,} samples from Laplace mixture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_overview(X_array_laplace, \"Laplace Mixture Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HD-LM model\n",
    "print(\"üèóÔ∏è Training HD-LM model...\")\n",
    "model_hdlm = HDlm()\n",
    "params_hdlm, stats_hdlm, config_hdlm = train_hd_model(model_hdlm, dataloader_laplace, config)\n",
    "print(f\"‚úÖ HD-LM training completed with reductions: {config_hdlm.reduction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate HD-LM\n",
    "aligned_params_hdlm, errors_hdlm = evaluate_model_performance(means_laplace, weights, covariances, params_hdlm, \"HD-LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison(means_laplace, weights, covariances, aligned_params_hdlm, errors_hdlm, \"HD-LM\", \"mediumseagreen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparative Analysis üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== FINAL COMPARISON ====================\n",
    "\n",
    "# Performance summary table\n",
    "models = [\"HD-GMM\", \"HD-STM\", \"HD-LM\"]\n",
    "mean_errors = [np.mean(errors_hdgmm), np.mean(errors_hdstm), np.mean(errors_hdlm)]\n",
    "weight_errors = [\n",
    "    np.linalg.norm(np.array(weights) - aligned_params_hdgmm.pi),\n",
    "    np.linalg.norm(np.array(weights) - aligned_params_hdstm.pi),\n",
    "    np.linalg.norm(np.array(weights) - aligned_params_hdlm.pi),\n",
    "]\n",
    "\n",
    "print(\"üèÜ FINAL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"{model:6} | Mean Error: {mean_errors[i]:.4f} | Weight Error: {weight_errors[i]:.4f}\")\n",
    "\n",
    "# Add degrees of freedom analysis for HD-STM\n",
    "nu_errors = [abs(aligned_params_hdstm.nu[i] - nus[i]) for i in range(3)]\n",
    "print(f\"\\nHD-STM degrees of freedom error: {np.mean(nu_errors):.4f}\")\n",
    "\n",
    "# Comprehensive visualization comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle(\"High-Dimensional Mixture Models: Comparative Performance\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# Mean reconstruction errors\n",
    "ax = axes[0, 0]\n",
    "bars = ax.bar(models, mean_errors, color=[\"lightcoral\", \"darkorange\", \"mediumseagreen\"], alpha=0.8)\n",
    "ax.set_ylabel(\"Mean L2 Error\")\n",
    "ax.set_title(\"Mean Vector Reconstruction Error\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + max(mean_errors) * 0.01,\n",
    "        f\"{height:.4f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "# Weight reconstruction errors\n",
    "ax = axes[0, 1]\n",
    "bars = ax.bar(models, weight_errors, color=[\"lightcoral\", \"darkorange\", \"mediumseagreen\"], alpha=0.8)\n",
    "ax.set_ylabel(\"Weight L2 Error\")\n",
    "ax.set_title(\"Mixing Weight Reconstruction Error\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + max(weight_errors) * 0.01,\n",
    "        f\"{height:.4f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "# Eigenvalue reconstruction quality (Component 1 example)\n",
    "ax = axes[0, 2]\n",
    "params_list = [aligned_params_hdgmm, aligned_params_hdstm, aligned_params_hdlm]\n",
    "colors = [\"lightcoral\", \"darkorange\", \"mediumseagreen\"]\n",
    "markers = [\"s\", \"^\", \"D\"]\n",
    "\n",
    "# Use Gaussian covariances for comparison (all models)\n",
    "true_eigenvals = np.sort(np.linalg.eigvals(covariances[0]))[::-1]\n",
    "x_pos = np.arange(len(true_eigenvals))\n",
    "ax.semilogy(x_pos, true_eigenvals, \"o-\", color=\"gold\", linewidth=3, markersize=8, label=\"True\")\n",
    "\n",
    "for i, (params, model, color, marker) in enumerate(zip(params_list, models, colors, markers)):\n",
    "    hd_A = np.array(params.A[0])\n",
    "    hd_b = params.b[0]\n",
    "    n_leading = len(hd_A)\n",
    "    n_tail = len(true_eigenvals) - n_leading\n",
    "    hd_eigenvals = np.concatenate([hd_A, np.full(n_tail, hd_b)])\n",
    "    hd_eigenvals = np.sort(hd_eigenvals)[::-1]\n",
    "    ax.semilogy(x_pos, hd_eigenvals, f\"{marker}-\", color=color, linewidth=2, markersize=6, label=model)\n",
    "\n",
    "ax.set_xlabel(\"Eigenvalue Index\")\n",
    "ax.set_ylabel(\"Eigenvalue (log scale)\")\n",
    "ax.set_title(\"Component 1 Eigenvalue Comparison\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Model-specific parameter analysis\n",
    "ax = axes[1, 0]\n",
    "# Reduction analysis\n",
    "reductions = [np.mean(config_hdgmm.reduction), np.mean(config_hdstm.reduction), np.mean(config_hdlm.reduction)]\n",
    "bars = ax.bar(models, reductions, color=[\"lightcoral\", \"darkorange\", \"mediumseagreen\"], alpha=0.8)\n",
    "ax.set_ylabel(\"Average Reduction\")\n",
    "ax.set_title(\"Average Dimensionality Reduction\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + max(reductions) * 0.01,\n",
    "        f\"{height:.1f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "# Degrees of freedom for HD-STM\n",
    "ax = axes[1, 1]\n",
    "ax.bar(\n",
    "    [\"True ŒΩ\", \"HD-STM ŒΩ\"], [np.mean(nus), np.mean(aligned_params_hdstm.nu)], color=[\"gold\", \"darkorange\"], alpha=0.8\n",
    ")\n",
    "ax.set_ylabel(\"Degrees of Freedom (ŒΩ)\")\n",
    "ax.set_title(\"HD-STM: Degrees of Freedom\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "for i, height in enumerate([np.mean(nus), np.mean(aligned_params_hdstm.nu)]):\n",
    "    ax.text(i, height + 0.1, f\"{height:.2f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "# Compression efficiency\n",
    "ax = axes[1, 2]\n",
    "# Calculate compression ratios\n",
    "total_params_full = [3 * 5 * (5 + 1) // 2 for _ in models]  # Full covariance matrices\n",
    "total_params_hd = [\n",
    "    sum(len(aligned_params_hdgmm.A[i]) for i in range(3)) + 3,  # A + b parameters\n",
    "    sum(len(aligned_params_hdstm.A[i]) for i in range(3)) + 3,  # A + b parameters\n",
    "    sum(len(aligned_params_hdlm.A[i]) for i in range(3)) + 3,  # A + b parameters\n",
    "]\n",
    "compression_ratios = [hd / full for hd, full in zip(total_params_hd, total_params_full)]\n",
    "\n",
    "bars = ax.bar(models, compression_ratios, color=[\"lightcoral\", \"darkorange\", \"mediumseagreen\"], alpha=0.8)\n",
    "ax.set_ylabel(\"Parameter Compression Ratio\")\n",
    "ax.set_title(\"Model Compression Efficiency\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2.0,\n",
    "        height + max(compression_ratios) * 0.01,\n",
    "        f\"{height:.2f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onlineem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
